{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a02bbe4e",
   "metadata": {},
   "source": [
    "### ðŸ“¦ Dataset Attribution\n",
    "\n",
    "This project uses the **FUTURA Synthetic Invoices Dataset**, publicly available via Zenodo:\n",
    "\n",
    "> **FUTURA - Synthetic Invoices Dataset for Document Analysis**  \n",
    "> Authors: Dimosthenis Karatzas, Fei Chen, Davide Fichera, Diego Marchetti  \n",
    "> License: [Creative Commons Attribution 4.0 International (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)  \n",
    "> DOI: [https://doi.org/10.5281/zenodo.10371464](https://doi.org/10.5281/zenodo.10371464)  \n",
    "> Accessed via Zenodo. Redistribution and derivative works must credit the original authors.\n",
    "\n",
    "We gratefully acknowledge the authors for creating and releasing this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e039af",
   "metadata": {},
   "source": [
    "## ðŸ§  Training CNN for Invoice Tampering Detection (with Class Imbalance Handling)\n",
    "\n",
    "In this notebook, we aim to train a Convolutional Neural Network using **PyTorch** to classify invoice images as either *real* or *tampered*. Given the dataset is **heavily imbalanced** (10,000 real vs 600 tampered), weâ€™ll take the following measures:\n",
    "\n",
    "- âœ… **Transfer Learning** using a pre-trained `ResNet18` to leverage learned visual features\n",
    "- ðŸ§ª **WeightedRandomSampler** to balance batches during training\n",
    "- ðŸŽ¨ **Data Augmentation** (e.g., random crops, flips, brightness changes) to increase tampered sample diversity\n",
    "- ðŸ“Š Track performance using train, validation, and test splits with confusion matrix and accuracy\n",
    "\n",
    "The overall goal is to train a reliable and reproducible model that detects subtle manipulations in invoices effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab9a9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, Seeding and Environment Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f1cee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# No GPU assumed for now; if available, still fine.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515035fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "def seed_all(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f197f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded X: shape=(10600, 841, 595, 3), dtype=uint8 (memory-mapped)\n",
      "âœ… Loaded y: shape=(10600,), unique labels: (array([0, 1]), array([10000,   600]))\n"
     ]
    }
   ],
   "source": [
    "# Load data using memory-mapping\n",
    "X = np.load(\"../data/processed/X.npy\", mmap_mode='r')\n",
    "y = np.load(\"../data/processed/y.npy\")  # y is small enough to load entirely\n",
    "\n",
    "print(f\"âœ… Loaded X: shape={X.shape}, dtype={X.dtype} (memory-mapped)\")\n",
    "print(f\"âœ… Loaded y: shape={y.shape}, unique labels: {np.unique(y, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a90606ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Train: 7420, Val: 1590, Test: 1590\n"
     ]
    }
   ],
   "source": [
    "# Split data into Train, Validation, and Test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: Train (70%) vs Temp (30%)\n",
    "X_train_idx, X_temp_idx, y_train, y_temp = train_test_split(\n",
    "    np.arange(len(X)), y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: Val (15%) and Test (15%)\n",
    "X_val_idx, X_test_idx, y_val, y_test = train_test_split(\n",
    "    X_temp_idx, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"ðŸ§ª Train: {len(X_train_idx)}, Val: {len(X_val_idx)}, Test: {len(X_test_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8678691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataLoader parameters\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Define target size for resizing\n",
    "TARGET_SIZE = (256, 256)\n",
    "\n",
    "# Define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(TARGET_SIZE),\n",
    "    transforms.ToTensor(),  # Converts to [C, H, W] with values in [0,1]\n",
    "])\n",
    "\n",
    "# Custom Dataset to read from memory-mapped array\n",
    "class InvoiceDataset(Dataset):\n",
    "    def __init__(self, X, y, indices, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        image = self.X[i]\n",
    "        label = self.y[i]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fff07bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Batch shape: torch.Size([32, 3, 256, 256]), Labels: tensor([0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Initialize datasets using index arrays (no slicing of X)\n",
    "train_dataset = InvoiceDataset(X, y, X_train_idx, transform)\n",
    "val_dataset   = InvoiceDataset(X, y, X_val_idx, transform)\n",
    "test_dataset  = InvoiceDataset(X, y, X_test_idx, transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Peek at one batch to verify\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"ðŸ§¾ Batch shape: {images.shape}, Labels: {labels[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "142d9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applied ONLY to training images\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(TARGET_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Used for validation and test (no changes to images)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(TARGET_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f201234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final train batch shape: torch.Size([32, 3, 256, 256]), Labels: tensor([0, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Redefine datasets with final transforms\n",
    "train_dataset = InvoiceDataset(X, y, X_train_idx, transform=train_transform)\n",
    "val_dataset   = InvoiceDataset(X, y, X_val_idx,   transform=eval_transform)\n",
    "test_dataset  = InvoiceDataset(X, y, X_test_idx,  transform=eval_transform)\n",
    "\n",
    "# Compute class weights: inverse of class frequency\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = class_weights[y_train]\n",
    "\n",
    "# Create the sampler\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Final DataLoaders using updated datasets and sampler\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=weighted_sampler)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Peek at final batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"âœ… Final train batch shape: {images.shape}, Labels: {labels[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "157cf510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ResNet18 adjusted for binary classification.\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 5 epochs\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load pre-trained ResNet18\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "model = resnet18(weights=weights)\n",
    "\n",
    "# Replace the final FC layer with a single output (for binary classification)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1)  # output = 1 logit\n",
    "\n",
    "model = model.to(device)\n",
    "print(\"âœ… ResNet18 adjusted for binary classification.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a24c55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification â†’ use BCEWithLogitsLoss (more numerically stable)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer (fine-tuning only the final layer is optional)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23b5b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().to(device).unsqueeze(1)  # for BCEWithLogitsLoss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.float().to(device).unsqueeze(1)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                correct += (preds.squeeze().long() == labels.squeeze().long()).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        print(f\"ðŸ“… Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | Val Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b8e2e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“… Epoch 1/5 | Train Loss: 0.1610 | Val Loss: 0.0627 | Val Accuracy: 0.9881\n",
      "ðŸ“… Epoch 2/5 | Train Loss: 0.1398 | Val Loss: 0.0763 | Val Accuracy: 0.9862\n",
      "ðŸ“… Epoch 3/5 | Train Loss: 0.1347 | Val Loss: 0.0999 | Val Accuracy: 0.9774\n",
      "ðŸ“… Epoch 4/5 | Train Loss: 0.1144 | Val Loss: 0.0771 | Val Accuracy: 0.9692\n",
      "ðŸ“… Epoch 5/5 | Train Loss: 0.1100 | Val Loss: 0.0796 | Val Accuracy: 0.9855\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 5 epochs\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7dd51c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Model saved to ../outputs/resnet_invoice.pt\n"
     ]
    }
   ],
   "source": [
    "# Make sure the actual directory exists\n",
    "os.makedirs(\"../outputs\", exist_ok=True)\n",
    "\n",
    "# Now save the model\n",
    "torch.save(model.state_dict(), \"../outputs/resnet_invoice.pt\")\n",
    "print(\"ðŸ’¾ Model saved to ../outputs/resnet_invoice.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d250dc8",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Conclusion: Invoice Fraud Detection Model\n",
    "\n",
    "This notebook trained a CNN using a fine-tuned **ResNet18** to detect tampered invoices.\n",
    "\n",
    "### âœ… Summary of Steps\n",
    "\n",
    "- **Data Handling**:\n",
    "  - Loaded preprocessed image arrays with `numpy.memmap`.\n",
    "  - Split data into train, val, and test sets.\n",
    "  - Applied augmentations for training; resizing for evaluation.\n",
    "\n",
    "- **Class Balancing**:\n",
    "  - Used class weights with a `WeightedRandomSampler`.\n",
    "\n",
    "- **Model**:\n",
    "  - Fine-tuned `ResNet18` for binary classification.\n",
    "  - Used `BCEWithLogitsLoss` + Adam optimizer.\n",
    "\n",
    "- **Training**:\n",
    "  - Trained for 5 epochs.\n",
    "  - Achieved **~98.55% validation accuracy**.\n",
    "\n",
    "- **Saving**:\n",
    "  - Saved model weights to `../outputs/resnet_invoice.pt`.\n",
    "\n",
    "This model is now ready for evaluation, inference, and deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
